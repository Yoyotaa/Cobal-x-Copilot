#!/usr/bin/env python3
"""
Tests de performance pour l'application comptable Python
Conforme aux standards RNCP - Niveau production
"""

import time
import statistics
import concurrent.futures
import pytest
import sys
import os
from unittest.mock import patch
import tempfile

# Ajouter le chemin du module
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from operations import Operations
from data import DataProgram

class PerformanceTestSuite:
    """Suite de tests de performance pour l'application comptable"""
    
    def __init__(self):
        self.setup_test_environment()
        
    def setup_test_environment(self):
        """Configuration de l'environnement de test"""
        self.temp_file = tempfile.NamedTemporaryFile(mode='w+', delete=False)
        self.temp_file.write('{"balance": 1000.0}')
        self.temp_file.close()
        
        self.operations = Operations()
        self.operations.data_program = DataProgram(self.temp_file.name)
        
    def measure_execution_time(self, func, *args, **kwargs):
        """Mesure le temps d'ex√©cution d'une fonction"""
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        return end_time - start_time, result
    
    def test_view_balance_performance(self, iterations=100):
        """Test de performance pour la consultation du solde"""
        print("\nüîç Test de performance - Consultation du solde")
        print("=" * 50)
        
        execution_times = []
        
        for i in range(iterations):
            with patch('builtins.print'):
                execution_time, result = self.measure_execution_time(
                    self.operations.view_balance
                )
                execution_times.append(execution_time)
        
        # Statistiques
        avg_time = statistics.mean(execution_times)
        min_time = min(execution_times)
        max_time = max(execution_times)
        median_time = statistics.median(execution_times)
        
        print(f"üìä R√©sultats pour {iterations} it√©rations :")
        print(f"   ‚è±Ô∏è  Temps moyen : {avg_time*1000:.2f} ms")
        print(f"   ‚ö° Temps minimum : {min_time*1000:.2f} ms")
        print(f"   üêå Temps maximum : {max_time*1000:.2f} ms")
        print(f"   üìê Temps m√©dian : {median_time*1000:.2f} ms")
        
        # Crit√®res de performance
        assert avg_time < 0.01, f"Performance d√©grad√©e : {avg_time*1000:.2f}ms > 10ms"
        assert max_time < 0.05, f"Pic de latence : {max_time*1000:.2f}ms > 50ms"
        
        return {
            'average': avg_time,
            'min': min_time,
            'max': max_time,
            'median': median_time
        }
    
    def test_credit_performance(self, iterations=500):
        """Test de performance pour les op√©rations de cr√©dit"""
        print("\nüí∞ Test de performance - Op√©rations de cr√©dit")
        print("=" * 50)
        
        execution_times = []
        
        for i in range(iterations):
            with patch('builtins.input', return_value='100.00'):
                with patch('builtins.print'):
                    execution_time, result = self.measure_execution_time(
                        self.operations.credit_account
                    )
                    execution_times.append(execution_time)
        
        avg_time = statistics.mean(execution_times)
        min_time = min(execution_times)
        max_time = max(execution_times)
        
        print(f"üìä R√©sultats pour {iterations} cr√©dits :")
        print(f"   ‚è±Ô∏è  Temps moyen : {avg_time*1000:.2f} ms")
        print(f"   ‚ö° Temps minimum : {min_time*1000:.2f} ms")
        print(f"   üêå Temps maximum : {max_time*1000:.2f} ms")
        
        # Crit√®res de performance pour les √©critures
        assert avg_time < 0.02, f"Performance √©criture d√©grad√©e : {avg_time*1000:.2f}ms > 20ms"
        
        return {'average': avg_time, 'min': min_time, 'max': max_time}
    
    def test_debit_performance(self, iterations=500):
        """Test de performance pour les op√©rations de d√©bit"""
        print("\nüí∏ Test de performance - Op√©rations de d√©bit")
        print("=" * 50)
        
        execution_times = []
        
        for i in range(iterations):
            with patch('builtins.input', return_value='50.00'):
                with patch('builtins.print'):
                    execution_time, result = self.measure_execution_time(
                        self.operations.debit_account
                    )
                    execution_times.append(execution_time)
        
        avg_time = statistics.mean(execution_times)
        min_time = min(execution_times)
        max_time = max(execution_times)
        
        print(f"üìä R√©sultats pour {iterations} d√©bits :")
        print(f"   ‚è±Ô∏è  Temps moyen : {avg_time*1000:.2f} ms")
        print(f"   ‚ö° Temps minimum : {min_time*1000:.2f} ms")
        print(f"   üêå Temps maximum : {max_time*1000:.2f} ms")
        
        assert avg_time < 0.02, f"Performance d√©bit d√©grad√©e : {avg_time*1000:.2f}ms > 20ms"
        
        return {'average': avg_time, 'min': min_time, 'max': max_time}
    
    def test_concurrent_operations(self, num_threads=5, operations_per_thread=10):
        """Test de performance sous charge concurrente"""
        print(f"\nüîÑ Test de performance - {num_threads} threads concurrents")
        print("=" * 50)
        
        def concurrent_operation():
            """Op√©ration √† ex√©cuter de mani√®re concurrente"""
            local_ops = Operations()
            local_ops.data_program = DataProgram(self.temp_file.name)
            
            start_time = time.perf_counter()
            
            for _ in range(operations_per_thread):
                with patch('builtins.print'):
                    local_ops.view_balance()
            
            return time.perf_counter() - start_time
        
        # Ex√©cution concurrente
        start_total = time.perf_counter()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(concurrent_operation) for _ in range(num_threads)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        total_time = time.perf_counter() - start_total
        total_operations = num_threads * operations_per_thread
        avg_throughput = total_operations / total_time
        
        print(f"üìä R√©sultats concurrence :")
        print(f"   üßµ Threads : {num_threads}")
        print(f"   üî¢ Total op√©rations : {total_operations}")
        print(f"   ‚è±Ô∏è  Temps total : {total_time:.2f}s")
        print(f"   ‚ö° D√©bit moyen : {avg_throughput:.0f} ops/sec")
        
        # Crit√®res de performance concurrentielle
        assert avg_throughput > 100, f"D√©bit insuffisant : {avg_throughput:.0f} < 100 ops/sec"
        
        return {
            'total_time': total_time,
            'throughput': avg_throughput,
            'operations': total_operations
        }
    
    def test_memory_usage(self, iterations=1000):
        """Test d'utilisation m√©moire"""
        print("\nüß† Test d'utilisation m√©moire")
        print("=" * 50)
        
        import psutil
        import gc
        
        # Mesure initiale
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Ex√©cution des op√©rations
        for i in range(iterations):
            with patch('builtins.print'):
                self.operations.view_balance()
                if i % 100 == 0:
                    gc.collect()  # Force garbage collection
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        print(f"üìä Utilisation m√©moire :")
        print(f"   üìè M√©moire initiale : {initial_memory:.1f} MB")
        print(f"   üìè M√©moire finale : {final_memory:.1f} MB")
        print(f"   üìà Augmentation : {memory_increase:.1f} MB")
        print(f"   üìä Par op√©ration : {memory_increase*1024/iterations:.2f} KB")
        
        # Crit√®re : pas plus de 50MB d'augmentation pour 1000 op√©rations
        assert memory_increase < 50, f"Fuite m√©moire d√©tect√©e : +{memory_increase:.1f}MB"
        
        return {
            'initial': initial_memory,
            'final': final_memory,
            'increase': memory_increase
        }
    
    def test_stress_test(self, duration_seconds=10):
        """Test de stress sur une dur√©e donn√©e"""
        print(f"\nüî• Test de stress - {duration_seconds} secondes")
        print("=" * 50)
        
        start_time = time.time()
        operations_count = 0
        errors_count = 0
        
        while time.time() - start_time < duration_seconds:
            try:
                with patch('builtins.print'):
                    self.operations.view_balance()
                operations_count += 1
            except Exception as e:
                errors_count += 1
                print(f"‚ùå Erreur : {e}")
        
        actual_duration = time.time() - start_time
        ops_per_second = operations_count / actual_duration
        error_rate = errors_count / operations_count if operations_count > 0 else 0
        
        print(f"üìä R√©sultats stress test :")
        print(f"   ‚è±Ô∏è  Dur√©e r√©elle : {actual_duration:.1f}s")
        print(f"   üî¢ Op√©rations : {operations_count}")
        print(f"   ‚ùå Erreurs : {errors_count}")
        print(f"   ‚ö° Ops/seconde : {ops_per_second:.0f}")
        print(f"   üìä Taux d'erreur : {error_rate*100:.2f}%")
        
        # Crit√®res de stress
        assert ops_per_second > 50, f"Performance stress insuffisante : {ops_per_second:.0f} < 50 ops/sec"
        assert error_rate < 0.01, f"Taux d'erreur trop √©lev√© : {error_rate*100:.2f}% > 1%"
        
        return {
            'duration': actual_duration,
            'operations': operations_count,
            'errors': errors_count,
            'ops_per_second': ops_per_second,
            'error_rate': error_rate
        }
    
    def generate_performance_report(self):
        """G√©n√®re un rapport complet de performance"""
        print("\n" + "="*60)
        print("üöÄ RAPPORT COMPLET DE PERFORMANCE")
        print("="*60)
        
        results = {}
        
        try:
            # Tests de base
            results['view_balance'] = self.test_view_balance_performance()
            results['credit'] = self.test_credit_performance()
            results['debit'] = self.test_debit_performance()
            
            # Tests avanc√©s
            results['concurrent'] = self.test_concurrent_operations()
            results['memory'] = self.test_memory_usage()
            results['stress'] = self.test_stress_test()
            
            # R√©sum√© global
            print("\nüìã R√âSUM√â GLOBAL")
            print("-" * 30)
            print("‚úÖ Tous les tests de performance r√©ussis")
            print(f"‚ö° D√©bit maximal : {results['concurrent']['throughput']:.0f} ops/sec")
            print(f"üß† Empreinte m√©moire : {results['memory']['increase']:.1f} MB/1000 ops")
            print(f"üéØ Performance stress : {results['stress']['ops_per_second']:.0f} ops/sec")
            
            # Sauvegarde du rapport
            self._save_performance_report(results)
            
        except Exception as e:
            print(f"‚ùå Erreur durant les tests : {e}")
            raise
        
        finally:
            self.cleanup()
    
    def _save_performance_report(self, results):
        """Sauvegarde le rapport de performance"""
        import json
        from datetime import datetime
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'results': results,
            'environment': {
                'python_version': sys.version,
                'platform': sys.platform
            }
        }
        
        with open('performance_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print("\nüíæ Rapport sauvegard√© : performance_report.json")
    
    def cleanup(self):
        """Nettoyage apr√®s les tests"""
        try:
            os.unlink(self.temp_file.name)
        except:
            pass

def main():
    """Point d'entr√©e principal"""
    print("üéØ SUITE DE TESTS DE PERFORMANCE")
    print("Application Comptable Python - Standards RNCP")
    print("=" * 60)
    
    # V√©rification des pr√©requis
    try:
        import psutil
    except ImportError:
        print("‚ùå Erreur : pip install psutil requis pour les tests m√©moire")
        sys.exit(1)
    
    # Ex√©cution de la suite de tests
    suite = PerformanceTestSuite()
    suite.generate_performance_report()
    
    print("\nüéâ Tests de performance termin√©s avec succ√®s !")

if __name__ == "__main__":
    main()
